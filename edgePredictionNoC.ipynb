{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251590e0-6fb8-483a-bd90-d38b6660220e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, EarlyStoppingCallback, DataCollatorForLanguageModeling\n",
    "import datasets\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4c562e1-e352-4c56-aa28-1f213599a32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58bd1630-ef7d-40c4-85df-b3ea77dad60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d12e1ffb-2ad8-40a5-8619-435f53de64a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "newpath = 'experiments/EXP' + EXP\n",
    "\n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "\n",
    "if not os.path.exists(newpath + 'model'):\n",
    "    os.makedirs(newpath + 'model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208b50ed-01bb-4c32-9de4-e2fc48614962",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a10ac209-a8dd-45b3-8faa-39fbf7e2c4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "graph = nx.read_graphml(\"../graph.graphml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5456ee-38de-4af0-9164-62f1182a1ffd",
   "metadata": {},
   "source": [
    "### Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c358a0d-5d1e-472a-8ee7-ee5e25145f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('experiments/EXP1/m_n_values.pkl', 'rb') as file:\n",
    "    m_n_values = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0a2b2d4-f18e-4247-ab7f-c0f286435802",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('experiments/EXP1/train_val_triples.pkl', 'rb') as file:\n",
    "    train_val_triples_community = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "437faa27-8eca-47b2-b698-9a1be7bc3370",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('experiments/EXP1/test_triples.pkl', 'rb') as file:\n",
    "    test_triples = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7e1e90-b7f1-42a4-9185-29744549312e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = []\n",
    "\n",
    "while len(train_set) < 5000:\n",
    "    t_list = []\n",
    "    node1 = str(random.randint(0, 129374))\n",
    "    g = graph[node1]\n",
    "    t_list.append(node1)\n",
    "    l = len(g)\n",
    "    i = 1\n",
    "    path_len = 5\n",
    "    while l > 0 and i < path_len and i <= l:\n",
    "        node2 = list(g)[random.randint(0, l-1)]\n",
    "        g = graph[node2]\n",
    "        t_list.append(node2)\n",
    "        l = len(g)\n",
    "        i += 1\n",
    "    if len(t_list) == 5:\n",
    "        train_set.append(t_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "21ab0576-e150-4bbd-9afb-faa3d9c7d7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in enumerate(train_set):\n",
    "    for n in t:\n",
    "        name = graph.nodes[n]['node_name']\n",
    "        for tt in test_triples:\n",
    "            if name == tt[0] or name == tt[2]:\n",
    "                train_set[i] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "070fa60e-22c4-400d-bef2-c35a3b9acc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_set = []\n",
    "\n",
    "for v in train_set:\n",
    "    if v != []:\n",
    "        t_set.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363db1fe-d733-45a5-bc70-4d9c3b406d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(t_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a5e59275-727a-4e68-941a-5609e44ffb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(t_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "348ce887-9179-4895-9e0c-5801584accfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = t_set[:1826]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b5424d-d08c-4815-ad47-c5ce232a705c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = []\n",
    "triples = set()\n",
    "\n",
    "for walk in train_set:\n",
    "    chat = []\n",
    "\n",
    "    system = {\"role\": \"system\", \"content\": \"You are a chatbot that has to predict the relationship between nodes.\"}\n",
    "    chat.append(system)\n",
    "    for i in range(len(walk)-1):\n",
    "        rel = graph[walk[i]][walk[i+1]]['display_relation']\n",
    "        a = graph.nodes[walk[i]]['node_name']\n",
    "        b = graph.nodes[walk[i+1]]['node_name']\n",
    "\n",
    "        triple = (a, rel, b)\n",
    "        triples.add(triple)\n",
    "        \n",
    "        user = \"Which is the relationship between the node '\" + a + \"' and the node '\" + b + \"'?\"\n",
    "        assistant = rel\n",
    "        \n",
    "        message = {}\n",
    "        message[\"role\"] = \"user\"\n",
    "        message[\"content\"] = user\n",
    "        chat.append(message)\n",
    "\n",
    "        message = {}\n",
    "        message[\"role\"] = \"assistant\"\n",
    "        message[\"content\"] = assistant\n",
    "        chat.append(message)\n",
    "\n",
    "    if chat not in train_dataset:\n",
    "        train_dataset.append(chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b094a047-0d44-4394-b7aa-843305752cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f4f94188-9d02-430b-a74b-51b168c88ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('experiments/EXP' + EXP + '/train_val_triples.pkl', 'wb') as file1:\n",
    "    pickle.dump(triples, file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cd5be901-a31b-4766-925a-d3c38faf4a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcea7010-9ccd-497e-be29-73c5d9237082",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = train_dataset[:182]\n",
    "train_dataset = train_dataset[182:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7452b7a5-a413-4af1-a70b-530ec9e404bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e86c0ad-bb91-417d-a9f2-1c18ce485a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "551a5e8e-2e30-4f15-9892-6f224365bd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('experiments/EXP' + EXP + '/train_dataset.pkl', 'wb') as file1:\n",
    "    pickle.dump(train_dataset, file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e31ba147-4282-47ff-b26d-19dd262cfa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('experiments/EXP' + EXP + '/val_dataset.pkl', 'wb') as file2:\n",
    "    pickle.dump(val_dataset, file2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbf9ee7-92f2-4163-b12b-961bc7bbdebe",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1baaf558-b3e3-4f9e-ab70-c7d3be9ac2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('experiments/EXP' + EXP + '/train_dataset.pkl', 'rb') as file:\n",
    "    train_dataset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5363cb1b-8690-402d-ae65-6a76c57f0494",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('experiments/EXP' + EXP + '/val_dataset.pkl', 'rb') as file:\n",
    "    val_dataset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5c41d426-561e-4d1b-81ff-f7c3f94f9514",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"sapienzanlp/Minerva-350m-base-v1.0\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"sapienzanlp/Minerva-350m-base-v1.0\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a395cb-ef80-4e33-a0ba-43e745d5248b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset1 = Dataset.from_dict({\"chat\": train_dataset})\n",
    "dataset1 = dataset1.map(lambda x: {\"formatted_chat\": tokenizer.apply_chat_template(x[\"chat\"], tokenize=True, add_generation_prompt=False)})\n",
    "\n",
    "dataset2 = Dataset.from_dict({\"chat\": val_dataset})\n",
    "dataset2 = dataset2.map(lambda x: {\"formatted_chat\": tokenizer.apply_chat_template(x[\"chat\"], tokenize=True, add_generation_prompt=False)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590def80-abf3-4bf5-b8d0-6018a0f75b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='outputs',\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=250,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    greater_is_better=False,\n",
    "    do_eval=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    eval_strategy=\"steps\",\n",
    "    logging_steps=250,\n",
    "    warmup_steps=50,\n",
    "    prediction_loss_only=True,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset1['formatted_chat'],\n",
    "    eval_dataset=dataset2['formatted_chat'],\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc7e2b2-4d78-46aa-9a48-17fc9b90ce85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('experiments/EXP' + EXP + 'model')\n",
    "tokenizer.save_pretrained('experiments/EXP' + EXP + 'model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21ae114-c204-4860-97ed-fb71f82f9211",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b403fbc4-16f9-4b08-812c-33853782c348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af420a52-0df4-423e-ac5e-0f510679deaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('experiments/EXP' + EXP + 'model')\n",
    "final_model = AutoModelForCausalLM.from_pretrained('experiments/EXP' + EXP + 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335a4f6c-1545-4bbe-9477-291563655f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('experiments/EXP1/test_dataset.pkl', 'rb') as file:\n",
    "    test_dataset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a6e7a3-144b-4627-a352-2167b9c6e896",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_ft = []\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    prompt = []\n",
    "    ground_truth = ''\n",
    "    for m in test_dataset[i]:\n",
    "        if m['role'] != 'assistant':\n",
    "            prompt.append(m)\n",
    "        else:\n",
    "            ground_truth = m['content']\n",
    "    inputs = tokenizer.apply_chat_template(prompt, add_generation_prompt=True, return_dict=True, return_tensors=\"pt\")\n",
    "    inputs = {k: v for k, v in inputs.items()}\n",
    "\n",
    "    if ground_truth != '':\n",
    "        tok = tokenizer(ground_truth, return_tensors=\"pt\")\n",
    "        out = final_model.generate(**inputs, max_new_tokens=len(tok['input_ids'][0]), do_sample=True, num_return_sequences=10)\n",
    "        generations = []\n",
    "        for j in range(10):\n",
    "            gen = tokenizer.decode(out[j][len(inputs[\"input_ids\"][0]):])\n",
    "            generations.append(gen)\n",
    "    \n",
    "        t = (test_dataset[i], generations)\n",
    "        post_ft.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced65462-ed91-45d8-872b-061fbfe0d8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('experiments/EXP' + EXP + '/results_postft.pkl', 'wb') as file:\n",
    "    pickle.dump(post_ft, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc14f42a-1535-4daf-a098-223682317a35",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "02872ac3-c11e-4be6-9b12-ff453ed15e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = []\n",
    "for d, g in post_ft:\n",
    "    gen = []\n",
    "    for pred in g:\n",
    "        gen.append(pred.strip())\n",
    "    clean.append((d, gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f00e24-580d-46f8-a641-d3f0ffea1416",
   "metadata": {},
   "outputs": [],
   "source": [
    "mrr_list = []\n",
    "for d, g in clean:\n",
    "    c = d[2]['content']\n",
    "    try:\n",
    "        mrr = g.index(c)\n",
    "    except ValueError:\n",
    "        mrr = 9\n",
    "    mrr_list.append(1/mrr)\n",
    "\n",
    "mrr_mean = sum(mrr_list) / len(mrr_list)\n",
    "mrr_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0372dbf8-6410-431a-af4b-8791d281d2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit1_list = []\n",
    "for d, g in clean:\n",
    "    c = d[2]['content']\n",
    "    if c == g[0]:\n",
    "        hit1_list.append(1)\n",
    "    else:\n",
    "        hit1_list.append(0)\n",
    "\n",
    "hit1_mean = sum(hit1_list) / len(hit1_list)\n",
    "\n",
    "hit3_list = []\n",
    "for d, g in clean:\n",
    "    c = d[2]['content']\n",
    "    if c in g[:3]:\n",
    "        hit3_list.append(1)\n",
    "    else:\n",
    "        hit3_list.append(0)\n",
    "\n",
    "hit3_mean = sum(hit3_list) / len(hit3_list)\n",
    "\n",
    "hit5_list = []\n",
    "for d, g in clean:\n",
    "    c = d[2]['content']\n",
    "    if c in g[:5]:\n",
    "        hit5_list.append(1)\n",
    "    else:\n",
    "        hit5_list.append(0)\n",
    "\n",
    "hit5_mean = sum(hit5_list) / len(hit5_list)\n",
    "\n",
    "hit7_list = []\n",
    "for d, g in clean:\n",
    "    c = d[2]['content']\n",
    "    if c in g[:7]:\n",
    "        hit7_list.append(1)\n",
    "    else:\n",
    "        hit7_list.append(0)\n",
    "\n",
    "hit7_mean = sum(hit7_list) / len(hit7_list)\n",
    "\n",
    "hit10_list = []\n",
    "for d, g in clean:\n",
    "    c = d[2]['content']\n",
    "    if c in g:\n",
    "        hit10_list.append(1)\n",
    "    else:\n",
    "        hit10_list.append(0)\n",
    "\n",
    "hit10_mean = sum(hit10_list) / len(hit10_list)\n",
    "\n",
    "print(\"Hit@1 = \" + str(hit1_mean))\n",
    "print(\"Hit@3 = \" + str(hit3_mean))\n",
    "print(\"Hit@5 = \" + str(hit5_mean))\n",
    "print(\"Hit@7 = \" + str(hit7_mean))\n",
    "print(\"Hit@10 = \" + str(hit10_mean))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
